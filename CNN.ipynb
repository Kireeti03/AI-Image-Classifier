{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Importing necessary libraries"
      ],
      "metadata": {
        "id": "kuy8Iv6uW8GU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiNZCY4YWbVF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking whether GPU is present and using it (T4 GPU)."
      ],
      "metadata": {
        "id": "9heoBLBMCkhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\n",
        "else:\n",
        "    print(\"No GPU available. Training will run on CPU.\")"
      ],
      "metadata": {
        "id": "2N4nIYQpXGwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "_JCBYZNgXmTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self, in_features, out_features):\n",
        "    super().__init__()\n"
      ],
      "metadata": {
        "id": "Cy_6pLtlCAXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldPcHhunOxR-",
        "outputId": "54073a30-24b0-4a98-d520-a443d62268c3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import nn, optim\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Paths to the dataset\n",
        "dataset_path = '/content/drive/MyDrive/dataset'\n",
        "ai_images_path = os.path.join(dataset_path, 'ai_images')\n",
        "real_images_path = os.path.join(dataset_path, 'real_images')\n",
        "\n",
        "# Commenting out the function to verify and clean images\n",
        "# def verify_images(path):\n",
        "#     for img_file in Path(path).rglob('*.*'):\n",
        "#         try:\n",
        "#             img = Image.open(img_file)\n",
        "#             img.verify()  # Verify if the file is a valid image\n",
        "#         except (IOError, SyntaxError) as e:\n",
        "#             print(f'Removing invalid image: {img_file}')\n",
        "#             os.remove(img_file)\n",
        "\n",
        "# Verify and clean images\n",
        "# verify_images(ai_images_path)\n",
        "# verify_images(real_images_path)\n",
        "\n",
        "# Function to check if the directories exist\n",
        "def check_dirs_exist(base_dir, categories):\n",
        "    train_dir = os.path.join(base_dir, 'train')\n",
        "    test_dir = os.path.join(base_dir, 'test')\n",
        "    for category in categories:\n",
        "        if not os.path.exists(os.path.join(train_dir, category)) or not os.path.exists(os.path.join(test_dir, category)):\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "# Create directories for train and test sets if they do not exist\n",
        "def create_split_dirs(base_dir, categories):\n",
        "    train_dir = os.path.join(base_dir, 'train')\n",
        "    test_dir = os.path.join(base_dir, 'test')\n",
        "    for category in categories:\n",
        "        os.makedirs(os.path.join(train_dir, category), exist_ok=True)\n",
        "        os.makedirs(os.path.join(test_dir, category), exist_ok=True)\n",
        "    return train_dir, test_dir\n",
        "\n",
        "# Split data into training and testing sets\n",
        "def split_data(source_dir, train_ratio=0.8):\n",
        "    files = [f for f in os.listdir(source_dir) if os.path.isfile(os.path.join(source_dir, f))]\n",
        "    random.shuffle(files)\n",
        "    train_size = int(len(files) * train_ratio)\n",
        "    train_files = files[:train_size]\n",
        "    test_files = files[train_size:]\n",
        "\n",
        "    return train_files, test_files\n",
        "\n",
        "# Move files to train and test directories\n",
        "def move_files(files, source_dir, dest_dir, category):\n",
        "    for file in files:\n",
        "        shutil.copy(os.path.join(source_dir, file), os.path.join(dest_dir, category, file))\n",
        "\n",
        "# Perform data splitting and organizing if train and test directories do not exist\n",
        "categories = ['ai_images', 'real_images']\n",
        "if not check_dirs_exist(dataset_path, categories):\n",
        "    print(\"Creating train and test directories and splitting data...\")\n",
        "    train_dir, test_dir = create_split_dirs(dataset_path, categories)\n",
        "\n",
        "    for category in categories:\n",
        "        source_dir = os.path.join(dataset_path, category)\n",
        "        train_files, test_files = split_data(source_dir)\n",
        "        move_files(train_files, source_dir, train_dir, category)\n",
        "        move_files(test_files, source_dir, test_dir, category)\n",
        "else:\n",
        "    print(\"Train and test directories already exist. Skipping data splitting...\")\n",
        "\n",
        "# Data normalization and augmentation\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((32, 32)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "        transforms.RandomAffine(0, translate=(0.1, 0.1)),\n",
        "        transforms.RandomGrayscale(p=0.1),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize((32, 32)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "# Create PyTorch datasets and dataloaders\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(dataset_path, x),\n",
        "                                          data_transforms[x])\n",
        "                  for x in ['train', 'test']}\n",
        "dataloaders = {x: DataLoader(image_datasets[x], batch_size=32, shuffle=True, num_workers=4)\n",
        "               for x in ['train', 'test']}\n",
        "\n",
        "# Get dataset sizes\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}\n",
        "\n",
        "# Get class names\n",
        "class_names = image_datasets['train'].classes\n",
        "\n",
        "print(f\"Classes: {class_names}\")\n",
        "print(f\"Dataset sizes: {dataset_sizes}\")\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 8, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(8 * 16 * 16, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "# Calculate the number of parameters to ensure they are within 10,000\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "# Initialize the model\n",
        "num_classes = len(class_names)  # Number of classes in the dataset\n",
        "model = SimpleCNN(num_classes=num_classes)\n",
        "num_params = count_parameters(model)\n",
        "print(f\"Number of parameters in the model: {num_params}\")\n",
        "\n",
        "# Ensure the number of parameters is within 10,000\n",
        "assert num_params <= 10000, \"The model has more than 10,000 parameters!\"\n",
        "\n",
        "# Use GPU if available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "\n",
        "# Training the model\n",
        "num_epochs = 25\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "    print('-' * 10)\n",
        "\n",
        "    for phase in ['train', 'test']:\n",
        "        if phase == 'train':\n",
        "            model.train()\n",
        "        else:\n",
        "            model.eval()\n",
        "\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "\n",
        "        for inputs, labels in dataloaders[phase]:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with torch.set_grad_enabled(phase == 'train'):\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                if phase == 'train':\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        if phase == 'train':\n",
        "            scheduler.step()\n",
        "\n",
        "        epoch_loss = running_loss / dataset_sizes[phase]\n",
        "        epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "        print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "print('Training complete')\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), 'cnn_model.pth')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgxNPU0Gew94",
        "outputId": "a39d1a2f-7605-4009-93b5-a4c19bed3126"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Train and test directories already exist. Skipping data splitting...\n",
            "Classes: ['ai_images', 'real_images']\n",
            "Dataset sizes: {'train': 623, 'test': 156}\n",
            "Number of parameters in the model: 4322\n",
            "Epoch 1/25\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.5160 Acc: 0.8154\n",
            "test Loss: 0.4500 Acc: 0.8462\n",
            "Epoch 2/25\n",
            "----------\n",
            "train Loss: 0.4388 Acc: 0.8475\n",
            "test Loss: 0.5003 Acc: 0.8333\n",
            "Epoch 3/25\n",
            "----------\n",
            "train Loss: 0.4505 Acc: 0.8427\n",
            "test Loss: 0.4488 Acc: 0.8462\n",
            "Epoch 4/25\n",
            "----------\n",
            "train Loss: 0.4168 Acc: 0.8475\n",
            "test Loss: 0.4839 Acc: 0.8141\n",
            "Epoch 5/25\n",
            "----------\n",
            "train Loss: 0.4157 Acc: 0.8443\n",
            "test Loss: 0.4575 Acc: 0.8205\n",
            "Epoch 6/25\n",
            "----------\n",
            "train Loss: 0.3861 Acc: 0.8523\n",
            "test Loss: 0.4144 Acc: 0.8462\n",
            "Epoch 7/25\n",
            "----------\n",
            "train Loss: 0.3870 Acc: 0.8459\n",
            "test Loss: 0.4629 Acc: 0.8269\n",
            "Epoch 8/25\n",
            "----------\n",
            "train Loss: 0.3942 Acc: 0.8427\n",
            "test Loss: 0.4417 Acc: 0.8526\n",
            "Epoch 9/25\n",
            "----------\n",
            "train Loss: 0.3951 Acc: 0.8411\n",
            "test Loss: 0.4486 Acc: 0.8462\n",
            "Epoch 10/25\n",
            "----------\n",
            "train Loss: 0.3814 Acc: 0.8379\n",
            "test Loss: 0.4396 Acc: 0.8333\n",
            "Epoch 11/25\n",
            "----------\n",
            "train Loss: 0.3939 Acc: 0.8475\n",
            "test Loss: 0.4490 Acc: 0.8333\n",
            "Epoch 12/25\n",
            "----------\n",
            "train Loss: 0.3895 Acc: 0.8427\n",
            "test Loss: 0.4514 Acc: 0.8333\n",
            "Epoch 13/25\n",
            "----------\n",
            "train Loss: 0.3888 Acc: 0.8475\n",
            "test Loss: 0.4491 Acc: 0.8397\n",
            "Epoch 14/25\n",
            "----------\n",
            "train Loss: 0.3935 Acc: 0.8443\n",
            "test Loss: 0.4458 Acc: 0.8462\n",
            "Epoch 15/25\n",
            "----------\n",
            "train Loss: 0.3808 Acc: 0.8507\n",
            "test Loss: 0.4475 Acc: 0.8526\n",
            "Epoch 16/25\n",
            "----------\n",
            "train Loss: 0.3707 Acc: 0.8555\n",
            "test Loss: 0.4465 Acc: 0.8462\n",
            "Epoch 17/25\n",
            "----------\n",
            "train Loss: 0.3795 Acc: 0.8459\n",
            "test Loss: 0.4452 Acc: 0.8462\n",
            "Epoch 18/25\n",
            "----------\n",
            "train Loss: 0.3810 Acc: 0.8443\n",
            "test Loss: 0.4467 Acc: 0.8462\n",
            "Epoch 19/25\n",
            "----------\n",
            "train Loss: 0.3882 Acc: 0.8459\n",
            "test Loss: 0.4463 Acc: 0.8462\n",
            "Epoch 20/25\n",
            "----------\n",
            "train Loss: 0.3712 Acc: 0.8507\n",
            "test Loss: 0.4469 Acc: 0.8462\n",
            "Epoch 21/25\n",
            "----------\n",
            "train Loss: 0.3863 Acc: 0.8379\n",
            "test Loss: 0.4447 Acc: 0.8462\n",
            "Epoch 22/25\n",
            "----------\n",
            "train Loss: 0.3850 Acc: 0.8491\n",
            "test Loss: 0.4449 Acc: 0.8462\n",
            "Epoch 23/25\n",
            "----------\n",
            "train Loss: 0.3783 Acc: 0.8427\n",
            "test Loss: 0.4449 Acc: 0.8462\n",
            "Epoch 24/25\n",
            "----------\n",
            "train Loss: 0.3841 Acc: 0.8459\n",
            "test Loss: 0.4448 Acc: 0.8462\n",
            "Epoch 25/25\n",
            "----------\n",
            "train Loss: 0.3733 Acc: 0.8491\n",
            "test Loss: 0.4448 Acc: 0.8462\n",
            "Training complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the transformations for inference\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load class names (adjust according to your dataset)\n",
        "class_names = ['ai_images', 'real_images']\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = SimpleCNN(num_classes=len(class_names))  # Ensure to instantiate the same model structure\n",
        "model.load_state_dict(torch.load('cnn_model.pth', map_location=torch.device('cpu')))\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "# Function to predict and display an image\n",
        "def predict_image(model, image_path, class_names):\n",
        "    image = Image.open(image_path)\n",
        "    image = data_transforms(image).unsqueeze(0)  # Add batch dimension\n",
        "    outputs = model(image)\n",
        "    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "    plt.imshow(image.squeeze().permute(1, 2, 0))  # Display the image\n",
        "    plt.title(f'Predicted: {class_names[preds[0]]}')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Example usage: Predict on new images\n",
        "new_images = [\n",
        "    '/path/to/your/new_image1.jpg',\n",
        "    '/path/to/your/new_image2.jpg'\n",
        "]\n",
        "\n",
        "for image_path in new_images:\n",
        "    predict_image(model, image_path, class_names)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5VIPFpCSxpE",
        "outputId": "b6466bae-c6da-4c05-ea93-b19b2b26eeb4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "----------\n",
            "train Loss: 1.9375 Acc: 0.7929\n",
            "test Loss: 0.4880 Acc: 0.8462\n",
            "Epoch 2/10\n",
            "----------\n",
            "train Loss: 0.4563 Acc: 0.8363\n",
            "test Loss: 0.4557 Acc: 0.8462\n",
            "Epoch 3/10\n",
            "----------\n",
            "train Loss: 0.4410 Acc: 0.8459\n",
            "test Loss: 0.4513 Acc: 0.8462\n",
            "Epoch 4/10\n",
            "----------\n",
            "train Loss: 0.4268 Acc: 0.8459\n",
            "test Loss: 0.4264 Acc: 0.8462\n",
            "Epoch 5/10\n",
            "----------\n",
            "train Loss: 0.4284 Acc: 0.8459\n",
            "test Loss: 0.4578 Acc: 0.8462\n",
            "Epoch 6/10\n",
            "----------\n",
            "train Loss: 0.4267 Acc: 0.8459\n",
            "test Loss: 0.3631 Acc: 0.8462\n",
            "Epoch 7/10\n",
            "----------\n",
            "train Loss: 0.4029 Acc: 0.8459\n",
            "test Loss: 0.3941 Acc: 0.8462\n",
            "Epoch 8/10\n",
            "----------\n",
            "train Loss: 0.3920 Acc: 0.8459\n",
            "test Loss: 0.3911 Acc: 0.8462\n",
            "Epoch 9/10\n",
            "----------\n",
            "train Loss: 0.4080 Acc: 0.8459\n",
            "test Loss: 0.3757 Acc: 0.8462\n",
            "Epoch 10/10\n",
            "----------\n",
            "train Loss: 0.4386 Acc: 0.8459\n",
            "test Loss: 0.4029 Acc: 0.8462\n",
            "Training complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the transformations for inference\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load class names (adjust according to your dataset)\n",
        "class_names = ['ai_images', 'real_images']\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = SimpleCNN(num_classes=len(class_names))  # Ensure to instantiate the same model structure\n",
        "model.load_state_dict(torch.load('/content/cnn_model.pth', map_location=torch.device('cpu')))\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "# Function to predict and display an image\n",
        "def predict_image(model, image_path, class_names):\n",
        "    image = Image.open(image_path)\n",
        "    image = data_transforms(image).unsqueeze(0)  # Add batch dimension\n",
        "    outputs = model(image)\n",
        "    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "    plt.imshow(image.squeeze().permute(1, 2, 0))  # Display the image\n",
        "    plt.title(f'Predicted: {class_names[preds[0]]}')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Example usage: Predict on new images\n",
        "new_images = [\n",
        "    '/content/Passport.jpg',\n",
        "    '/content/midjourney_269.jpg',\n",
        "    '/content/20230115_141423.jpg'\n",
        "    # '/path/to/your/new_image2.jpg'\n",
        "]\n",
        "\n",
        "for image_path in new_images:\n",
        "    predict_image(model, image_path, class_names)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NSbs_q0UV1LU",
        "outputId": "dcf659ae-b280-4a9b-8356-5af8601ed93e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define the CNN architecture\n",
        "class CustomCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.fc1 = nn.Linear(128 * 32 * 32, 512)  # Assuming input images are 128x128\n",
        "        self.fc2 = nn.Linear(512, 2)  # Binary classification (AI-generated vs real)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 128 * 32 * 32)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Define the data transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "# Load the dataset\n",
        "train_dataset = ImageFolder(root='path/to/train_data', transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = CustomCNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(10):  # Number of epochs\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}')\n",
        "\n",
        "print('Finished Training')\n"
      ],
      "metadata": {
        "id": "9E3db1CPcJ4U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}